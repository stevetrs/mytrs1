v2.3(2023.7.18):
1.新增翻译引擎deepl，config设置项为
    "deepl": {
        "dic": "dict.json",
        "authorization": "相当于百度的id+key",
        "source_lang": "JA",源语言
        "target_lang": "ZH",目标语言
        "NPQ": 5000,每次请求最大字节，官方文档上写的是128mb，也就是128*1024，我没试过，具体不清楚
        "glossary_id": false,术语库id，建议新建一个源语言和目标语言都是"spr"中的设置项的术语词条，否则换行符可能会丢失
        "limits": 450000,
        "usage": 0  我没找到官方说明，不清楚字符是怎么计算的，所以字符统计不是很准，最好留出余量
    }
    目前发现的问题:
    1.有部分译文会是英文，不清楚为什么，我也不好处理
    2.字符统计不准
    3.在不设置术语库的情况下，换行符会有丢失
    本人目前没有deepl账号，所以对deepl的测试，调试不会很充分，这部分就靠广大网友了
v2.2.2(2023.7.7):
1.修复了2.2版本导致的，当引擎消耗达到上限时不会自动切换到下一个引擎的bug
2.修复了config文件的enginlist会自动剔除已达上限的引擎的bug
v2.2.1(2023.7.7):
1.读取字典文件失败不会导致翻译数据丢失了
v2.2(2023.7.2):
1.新增正在使用的引擎提示
2.微调部分代码，注：如果发现翻译进度一直重复，不要犹豫，立刻关掉程序，然后上报bug
3.针对百度引擎的bug进行优化（咋就百度这么特殊呢），现在当出现部分可继续翻译的错误代码时，不会直接中断百度引擎的翻译了
v2.1.1(2023.6.16):
1.修复了没有异常文本也会遍历backupengine的bug
2.修复了我以为我修复了百度统计消耗量不准的bug的bug及其附属bug
v2.1(2023.6.16):
1.修改了字符统计总量的计算方法，现在以百度和小牛的为准了（以前是以彩云为准）
v2.0(2023.6.16):
1.重写工具，不再使用pandas库
2.修复了百度引擎统计消耗量不符的bug
3.dic功能变更，现在为在引擎翻译结束后，参照mtool的mods中的翻译修正规则，替换译文。设为0或false则不使用此功能。
具体dic文件规则为
{
    "B1":{
        "C1":"A1",
        "C2":"A2",
        "C3":""
    },
    "B2":{
        "C1":"A1",
        "C2":"A2",
        "C3":""
    },
}
对于所有译文中含“B”的，若原文中含“A”，则将译文中的“B”替换为“C”，“A”可为空（""），此时不检测原文，
“A”为空的一定要放在这个B的大括号的最后，否则后续CA组合都不会生效。
一般来说，不同翻译引擎的“B”不同，所以dic文件未必通用，我提供的dict.json是百度引擎的。
v1.9.8(2023.6.14):
(需更新config)更新config方法：按照已有格式，把config新增项放在任意位置即可，所有数据类型的注意最后一行不要有逗号
config新增项：
{
    "src_rep": 数字,
    "dst_rep": 数字,
    "backupengine": [
        "caiyun",
        "xiaoniu"（比如说这里就没有逗号）
    ](全文件的末尾也没有逗号）
}
1.增加全新功能，能处理翻译结果中有大量连续重复字符时的情况，比如这种
    "「――ひぃぃあぁぁあッ！！」": "“——啊啊啊啊啊！！”",
    "「おっせーなーサオリの奴\nどこまでオヤツ買いに行ったんだ？」": "“喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂",
    "なあスケベしようか": "喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂喂",
处理方法为，在全部翻译完成后，遍历整个trsdata，将译文中有连续重复dst_rep个以上的文字，并且原文中连续重复文字不超过
src_rep个的文本重新用"backupengine"中的引擎依次重新翻译一遍。如果到最后仍有这种文本，会导出为repeat.json文件以供
手动处理，如果"backupengine"是空的（[]），就只会导出异常文本。
应用示例：
正常用baidu翻译了全部文本，然后判断到出现了异常文本，会先用"backupengine"中的第一个引擎，比如说caiyun，对这些
文本重新翻译，翻译后，如果还存在，就继续用第二个引擎xiaoniu再翻译一次，如果还有，导出剩余的异常文本
v1.9.7(2023.6.13):
1.修复了cleaner工作时会跳过最后一行字符串的bug（很好奇当时我是出于什么想法才再循环上加了个-1的）
2.修复了报错或字符数不足时，在TrsData内会有一行未翻译数据的问题
3.添加了工具图标
v1.9.6(2023.6.12):
1.cleaner开始使用正则表达式判断，不再需要手动列举特殊字符了，并且添加英文模式
现在的cleaner工作逻辑为：
日文模式下，会自动剔除所有不含日文假名的字符串（只有日文汉字的也会被剔除）
英文模式下，会自动剔除所有不含大小写字母的字符串（换行符\r等不会被认作是字母）
untrsfix同步更新，在没有config文件时，默认是日文模式
v1.9.5.2(2023.6.5):
1.用来替换\r\n的分隔符，现在可以在config里自由调整了
2.导出失败时，将数据输出改为Excel文件
v1.9.5(2023.6.4):
1.更新剔除中文的功能，只在cleaner开始时工作。目的为剔除trs_extractor处理后，残留的部分已汉化文本，推荐平时关闭，因为它也会将日文汉字识别为中文。
v1.9.4(2023.5.11):
1.现在文本过长也不会返回空参数了，但是如果超出引擎限制，可能会返回其他错误代码（每次请求的第一行字符串会无视NPQ的限制进行翻译）
v1.9.3（2023.5.10）：
1.现在会输出完整的报错信息了，方便各位上报bug，也方便我筛查问题所在
v1.9.2（2023.5.7）：
1.修复了会遍历整个enginelist，重复翻译的问题
v1.9.1（2023.5.7）:
1.更新了百度api的请求网址
v1.9（2023.4.24）:
1.config添加一项"cleaner"，用于开关cleaner功能，0关闭，1开启。对于英文翻译，不能使用cleaner功能，用了的话会把英文文本全部删除，以前没注意到。
并将进行判断的特殊字符放在了config的"special"中，可自行删改
2.优化了每次请求中字符量（NPQ）的判断，现在每次请求一定不会超过NPQ了，同时对limits的判断也更为精确，实际消耗会控制在limits-NPQ左右
3.修正了在1.7和1.8更新后发现的bug

v1.8（2023.4.24）:
1.现在在翻译前会先依照文本长短进行排序，优先翻译长文本，因为长文本大概率是剧情文本，这样在翻译引擎字数不足时，这些文本会被优先翻译。
在这种情况下，推荐将翻译效果好的引擎放在enginelist的前面，一种进阶玩法是，手动调整limits的大小，达到用效果比较好的翻译引擎将大部分剧情文本
翻译后，用效果差的翻译引擎去翻译那些不重要的文本。（怎么可能有人能靠手动设置做到这个-_-,用代码比较好的实现这个效果的方法我也没想到，以后再说吧）
（其实这个功能是想用在chatgpt上的，奈何gpt没有免费额度，只能作罢。）

v1.7（2023.4.24）:
1.更换了分隔每行字符的分隔符（从“S/som”换成了“↑☆↓☆”），消除了可能的对英文翻译的影响；
2.现在能够自动记录api通过本工具消耗的总字符量，并且每月初自动重置；
3.删除了config.json中‘engine’项，改为按‘enginelist’中的先后顺序，按顺序使用翻译引擎，当前一个引擎字数达到上限后，自动更换到下一个引擎
若所有引擎的总余量仍不足以翻译整个文件，则会在翻译前提示，并在翻译后将已翻译部分和未翻译部分分别保存在TrsData.bin和untrsed.json中，
配置文件config.json的说明已在“必看！！使用说明.txt”中更新。

v1.6:

1.大概率修正了彩云吞换行符的问题

v1.5：
1.因为百度翻译存在奇怪的bug且找不到原因，直接修改了整个框架，现在的框架比以前的更合理了
2.修正了换行符只考虑到\n而没考虑到\r的问题
3.修正了读取文件失败后闪退的问题（应该停留在报错界面）

4.修正了字词库替换原文后没有替换回来的问题